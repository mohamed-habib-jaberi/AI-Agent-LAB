{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80c62f61-a52a-41db-bc7c-b42da7226146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0092c968-e55d-4381-b0f7-6b9c045fab16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/JABERI/Documents/Dev/AI Agent/KodeKloud/AI Agent/AI-Agent-LAB/.venv/lib/python3.14/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/JABERI/Documents/Dev/AI Agent/KodeKloud/AI Agent/AI-Agent-LAB/.venv/lib/python3.14/site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/JABERI/Documents/Dev/AI Agent/KodeKloud/AI Agent/AI-Agent-LAB/.venv/lib/python3.14/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/JABERI/Documents/Dev/AI Agent/KodeKloud/AI Agent/AI-Agent-LAB/.venv/lib/python3.14/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/JABERI/Documents/Dev/AI Agent/KodeKloud/AI Agent/AI-Agent-LAB/.venv/lib/python3.14/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/JABERI/Documents/Dev/AI Agent/KodeKloud/AI Agent/AI-Agent-LAB/.venv/lib/python3.14/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8da2931f-51c5-4080-88e2-117a4a66bc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error getting response: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************OOYA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}. (request_id: req_cf30f81ab76d45ff92b06d7c8b45944d)\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************OOYA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdownloads_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Run the async function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m stock \u001b[38;5;129;01min\u001b[39;00m fav_stock:\n\u001b[32m     19\u001b[39m     query = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSearch the web for news about \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstock\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m and give me 1 recent update in a sentence\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(agent, query)\n\u001b[32m     21\u001b[39m     summary = result.final_output.strip()\n\u001b[32m     23\u001b[39m     sentiment_query = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWhat is the sentiment of this sentence? \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m Answer only with: positive, neutral, or negative.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev/AI Agent/KodeKloud/AI Agent/AI-Agent-LAB/.venv/lib/python3.14/site-packages/agents/run.py:358\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, conversation_id, session)\u001b[39m\n\u001b[32m    309\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    310\u001b[39m \u001b[33;03mRun a workflow starting at the given agent.\u001b[39;00m\n\u001b[32m    311\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    354\u001b[39m \u001b[33;03m    type of the output.\u001b[39;00m\n\u001b[32m    355\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    357\u001b[39m runner = DEFAULT_AGENT_RUNNER\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m runner.run(\n\u001b[32m    359\u001b[39m     starting_agent,\n\u001b[32m    360\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    361\u001b[39m     context=context,\n\u001b[32m    362\u001b[39m     max_turns=max_turns,\n\u001b[32m    363\u001b[39m     hooks=hooks,\n\u001b[32m    364\u001b[39m     run_config=run_config,\n\u001b[32m    365\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    366\u001b[39m     conversation_id=conversation_id,\n\u001b[32m    367\u001b[39m     session=session,\n\u001b[32m    368\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev/AI Agent/KodeKloud/AI Agent/AI-Agent-LAB/.venv/lib/python3.14/site-packages/agents/run.py:638\u001b[39m, in \u001b[36mAgentRunner.run\u001b[39m\u001b[34m(self, starting_agent, input, **kwargs)\u001b[39m\n\u001b[32m    630\u001b[39m     sequential_results = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    631\u001b[39m         starting_agent,\n\u001b[32m    632\u001b[39m         sequential_guardrails,\n\u001b[32m    633\u001b[39m         _copy_str_or_list(prepared_input),\n\u001b[32m    634\u001b[39m         context_wrapper,\n\u001b[32m    635\u001b[39m     )\n\u001b[32m    637\u001b[39m \u001b[38;5;66;03m# Run parallel guardrails + agent together.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m638\u001b[39m input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    639\u001b[39m     \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    640\u001b[39m         starting_agent,\n\u001b[32m    641\u001b[39m         parallel_guardrails,\n\u001b[32m    642\u001b[39m         _copy_str_or_list(prepared_input),\n\u001b[32m    643\u001b[39m         context_wrapper,\n\u001b[32m    644\u001b[39m     ),\n\u001b[32m    645\u001b[39m     \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    646\u001b[39m         agent=current_agent,\n\u001b[32m    647\u001b[39m         all_tools=all_tools,\n\u001b[32m    648\u001b[39m         original_input=original_input,\n\u001b[32m    649\u001b[39m         generated_items=generated_items,\n\u001b[32m    650\u001b[39m         hooks=hooks,\n\u001b[32m    651\u001b[39m         context_wrapper=context_wrapper,\n\u001b[32m    652\u001b[39m         run_config=run_config,\n\u001b[32m    653\u001b[39m         should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    654\u001b[39m         tool_use_tracker=tool_use_tracker,\n\u001b[32m    655\u001b[39m         server_conversation_tracker=server_conversation_tracker,\n\u001b[32m    656\u001b[39m     ),\n\u001b[32m    657\u001b[39m )\n\u001b[32m    659\u001b[39m \u001b[38;5;66;03m# Combine sequential and parallel results.\u001b[39;00m\n\u001b[32m    660\u001b[39m input_guardrail_results = sequential_results + input_guardrail_results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev/AI Agent/KodeKloud/AI Agent/AI-Agent-LAB/.venv/lib/python3.14/site-packages/agents/run.py:1550\u001b[39m, in \u001b[36mAgentRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, server_conversation_tracker)\u001b[39m\n\u001b[32m   1547\u001b[39m     \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m   1548\u001b[39m     \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m   1551\u001b[39m     agent,\n\u001b[32m   1552\u001b[39m     system_prompt,\n\u001b[32m   1553\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1554\u001b[39m     output_schema,\n\u001b[32m   1555\u001b[39m     all_tools,\n\u001b[32m   1556\u001b[39m     handoffs,\n\u001b[32m   1557\u001b[39m     hooks,\n\u001b[32m   1558\u001b[39m     context_wrapper,\n\u001b[32m   1559\u001b[39m     run_config,\n\u001b[32m   1560\u001b[39m     tool_use_tracker,\n\u001b[32m   1561\u001b[39m     server_conversation_tracker,\n\u001b[32m   1562\u001b[39m     prompt_config,\n\u001b[32m   1563\u001b[39m )\n\u001b[32m   1565\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m   1566\u001b[39m     agent=agent,\n\u001b[32m   1567\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1576\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m   1577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev/AI Agent/KodeKloud/AI Agent/AI-Agent-LAB/.venv/lib/python3.14/site-packages/agents/run.py:1807\u001b[39m, in \u001b[36mAgentRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, hooks, context_wrapper, run_config, tool_use_tracker, server_conversation_tracker, prompt_config)\u001b[39m\n\u001b[32m   1798\u001b[39m previous_response_id = (\n\u001b[32m   1799\u001b[39m     server_conversation_tracker.previous_response_id\n\u001b[32m   1800\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m server_conversation_tracker\n\u001b[32m   1801\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1802\u001b[39m )\n\u001b[32m   1803\u001b[39m conversation_id = (\n\u001b[32m   1804\u001b[39m     server_conversation_tracker.conversation_id \u001b[38;5;28;01mif\u001b[39;00m server_conversation_tracker \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1805\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1807\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m   1808\u001b[39m     system_instructions=filtered.instructions,\n\u001b[32m   1809\u001b[39m     \u001b[38;5;28minput\u001b[39m=filtered.input,\n\u001b[32m   1810\u001b[39m     model_settings=model_settings,\n\u001b[32m   1811\u001b[39m     tools=all_tools,\n\u001b[32m   1812\u001b[39m     output_schema=output_schema,\n\u001b[32m   1813\u001b[39m     handoffs=handoffs,\n\u001b[32m   1814\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m   1815\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m   1816\u001b[39m     ),\n\u001b[32m   1817\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m   1818\u001b[39m     conversation_id=conversation_id,\n\u001b[32m   1819\u001b[39m     prompt=prompt_config,\n\u001b[32m   1820\u001b[39m )\n\u001b[32m   1822\u001b[39m context_wrapper.usage.add(new_response.usage)\n\u001b[32m   1824\u001b[39m \u001b[38;5;66;03m# If we have run hooks, or if the agent has hooks, we need to call them after the LLM call\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev/AI Agent/KodeKloud/AI Agent/AI-Agent-LAB/.venv/lib/python3.14/site-packages/agents/models/openai_responses.py:92\u001b[39m, in \u001b[36mOpenAIResponsesModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id, conversation_id, prompt)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m response_span(disabled=tracing.is_disabled()) \u001b[38;5;28;01mas\u001b[39;00m span_response:\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m     93\u001b[39m             system_instructions,\n\u001b[32m     94\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m     95\u001b[39m             model_settings,\n\u001b[32m     96\u001b[39m             tools,\n\u001b[32m     97\u001b[39m             output_schema,\n\u001b[32m     98\u001b[39m             handoffs,\n\u001b[32m     99\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m    100\u001b[39m             conversation_id=conversation_id,\n\u001b[32m    101\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    102\u001b[39m             prompt=prompt,\n\u001b[32m    103\u001b[39m         )\n\u001b[32m    105\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _debug.DONT_LOG_MODEL_DATA:\n\u001b[32m    106\u001b[39m             logger.debug(\u001b[33m\"\u001b[39m\u001b[33mLLM responded\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev/AI Agent/KodeKloud/AI Agent/AI-Agent-LAB/.venv/lib/python3.14/site-packages/agents/models/openai_responses.py:308\u001b[39m, in \u001b[36mOpenAIResponsesModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, previous_response_id, conversation_id, stream, prompt)\u001b[39m\n\u001b[32m    304\u001b[39m         response_format = {\u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: model_settings.verbosity}\n\u001b[32m    306\u001b[39m stream_param: Literal[\u001b[38;5;28;01mTrue\u001b[39;00m] | Omit = \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m omit\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.responses.create(\n\u001b[32m    309\u001b[39m     previous_response_id=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(previous_response_id),\n\u001b[32m    310\u001b[39m     conversation=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(conversation_id),\n\u001b[32m    311\u001b[39m     instructions=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(system_instructions),\n\u001b[32m    312\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    313\u001b[39m     \u001b[38;5;28minput\u001b[39m=list_input,\n\u001b[32m    314\u001b[39m     include=include,\n\u001b[32m    315\u001b[39m     tools=converted_tools_payload,\n\u001b[32m    316\u001b[39m     prompt=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(prompt),\n\u001b[32m    317\u001b[39m     temperature=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.temperature),\n\u001b[32m    318\u001b[39m     top_p=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.top_p),\n\u001b[32m    319\u001b[39m     truncation=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.truncation),\n\u001b[32m    320\u001b[39m     max_output_tokens=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.max_tokens),\n\u001b[32m    321\u001b[39m     tool_choice=tool_choice,\n\u001b[32m    322\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    323\u001b[39m     stream=cast(Any, stream_param),\n\u001b[32m    324\u001b[39m     extra_headers=\u001b[38;5;28mself\u001b[39m._merge_headers(model_settings),\n\u001b[32m    325\u001b[39m     extra_query=model_settings.extra_query,\n\u001b[32m    326\u001b[39m     extra_body=model_settings.extra_body,\n\u001b[32m    327\u001b[39m     text=response_format,\n\u001b[32m    328\u001b[39m     store=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.store),\n\u001b[32m    329\u001b[39m     prompt_cache_retention=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.prompt_cache_retention),\n\u001b[32m    330\u001b[39m     reasoning=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.reasoning),\n\u001b[32m    331\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.metadata),\n\u001b[32m    332\u001b[39m     **extra_args,\n\u001b[32m    333\u001b[39m )\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(Union[Response, AsyncStream[ResponseStreamEvent]], response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev/AI Agent/KodeKloud/AI Agent/AI-Agent-LAB/.venv/lib/python3.14/site-packages/openai/resources/responses/responses.py:2320\u001b[39m, in \u001b[36mAsyncResponses.create\u001b[39m\u001b[34m(self, background, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, prompt_cache_retention, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2282\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2283\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2284\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2318\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   2319\u001b[39m ) -> Response | AsyncStream[ResponseStreamEvent]:\n\u001b[32m-> \u001b[39m\u001b[32m2320\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/responses\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2322\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2323\u001b[39m             {\n\u001b[32m   2324\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mbackground\u001b[39m\u001b[33m\"\u001b[39m: background,\n\u001b[32m   2325\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mconversation\u001b[39m\u001b[33m\"\u001b[39m: conversation,\n\u001b[32m   2326\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m: include,\n\u001b[32m   2327\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2328\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minstructions\u001b[39m\u001b[33m\"\u001b[39m: instructions,\n\u001b[32m   2329\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_output_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_output_tokens,\n\u001b[32m   2330\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: max_tool_calls,\n\u001b[32m   2331\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2332\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2333\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2334\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprevious_response_id\u001b[39m\u001b[33m\"\u001b[39m: previous_response_id,\n\u001b[32m   2335\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt,\n\u001b[32m   2336\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2337\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_retention\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_retention,\n\u001b[32m   2338\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m: reasoning,\n\u001b[32m   2339\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2340\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2341\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2342\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2343\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2344\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2345\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: text,\n\u001b[32m   2346\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2347\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2348\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2349\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2350\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtruncation\u001b[39m\u001b[33m\"\u001b[39m: truncation,\n\u001b[32m   2351\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2352\u001b[39m             },\n\u001b[32m   2353\u001b[39m             response_create_params.ResponseCreateParamsStreaming\n\u001b[32m   2354\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2355\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m response_create_params.ResponseCreateParamsNonStreaming,\n\u001b[32m   2356\u001b[39m         ),\n\u001b[32m   2357\u001b[39m         options=make_request_options(\n\u001b[32m   2358\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2359\u001b[39m         ),\n\u001b[32m   2360\u001b[39m         cast_to=Response,\n\u001b[32m   2361\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2362\u001b[39m         stream_cls=AsyncStream[ResponseStreamEvent],\n\u001b[32m   2363\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev/AI Agent/KodeKloud/AI Agent/AI-Agent-LAB/.venv/lib/python3.14/site-packages/openai/_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1780\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1782\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1789\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1790\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1791\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev/AI Agent/KodeKloud/AI Agent/AI-Agent-LAB/.venv/lib/python3.14/site-packages/openai/_base_client.py:1594\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1591\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1593\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1598\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************OOYA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[non-fatal] Tracing client error 401: {\n",
      "  \"error\": {\n",
      "    \"message\": \"Incorrect API key provided: sk-proj-********************************************************************************************************************************************************OOYA. You can find your API key at https://platform.openai.com/account/api-keys.\",\n",
      "    \"type\": \"invalid_request_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": \"invalid_api_key\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from agents import Agent, Runner, WebSearchTool, trace\n",
    "\n",
    "fav_stock = [\"Google\", \"Apple\", \"Nvidia\"]\n",
    "\n",
    "async def main():\n",
    "    agent = Agent(\n",
    "        name=\"Stock News Expert\",\n",
    "        instructions=\"You are a stock news genius. Review the most recent news on these stocks/companies.\",\n",
    "        tools=[WebSearchTool(user_location={\"type\": \"approximate\", \"city\": \"New York City\"})],\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with trace(\"Stock news summary\"):\n",
    "        for stock in fav_stock:\n",
    "            query = f\"Search the web for news about '{stock}' and give me 1 recent update in a sentence\"\n",
    "            result = await Runner.run(agent, query)\n",
    "            summary = result.final_output.strip()\n",
    "\n",
    "            sentiment_query = f\"What is the sentiment of this sentence? '{summary}' Answer only with: positive, neutral, or negative.\"\n",
    "            sentiment_result = await Runner.run(agent, sentiment_query)\n",
    "            sentiment = sentiment_result.final_output.strip()\n",
    "\n",
    "            sentiment_emoji = {\n",
    "                \"positive\": \"üìà\",\n",
    "                \"neutral\": \"üòê\",\n",
    "                \"negative\": \"üö®\"\n",
    "            }\n",
    "\n",
    "            sentiment_display = f\"{sentiment} {sentiment_emoji.get(sentiment.lower(), '')}\"\n",
    "\n",
    "            link = None\n",
    "            if hasattr(result, \"sources\") and result.sources:\n",
    "                link = result.sources[0].get(\"url\")\n",
    "\n",
    "            results.append({\n",
    "                \"Stock\": stock,\n",
    "                \"News\": summary,\n",
    "                \"Sentiment\": sentiment_display,\n",
    "                \"Link\": link\n",
    "            })\n",
    "\n",
    "    # ‚¨áÔ∏è Excel Export (correct indentation!)\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_excel(\"Stock_News_Summary_Pro.xlsx\", index=False)\n",
    "    print(\"News saved to Stock_News_Summary_Pro.xlsx\")\n",
    "\n",
    "    downloads_path = Path.home() / \"Downloads\" / \"Stock_News_Summary_Pro.xlsx\"\n",
    "    df.to_excel(downloads_path, index=False)\n",
    "    print(f\"Saved to {downloads_path}\")\n",
    "\n",
    "\n",
    "# Run the async function\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16600894-0cc0-4e9d-8395-f88477257317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
